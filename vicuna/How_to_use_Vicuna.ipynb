{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 安装环境"
      ],
      "metadata": {
        "id": "i25kEazRanJA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj2Nd4dIuGgB"
      },
      "outputs": [],
      "source": [
        "!pip3 install fschat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyllama -U"
      ],
      "metadata": {
        "id": "-vJ7U4TfyHTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 设置根目录"
      ],
      "metadata": {
        "id": "zCbjsMgkq2lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Up74qnrq0ly",
        "outputId": "16ef0413-d74f-4e16-c732-eb5fbaca05c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/drive/MyDrive/vicuna'"
      ],
      "metadata": {
        "id": "VDWlJwpSA9Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/vicuna')\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOjfI2-FaPel",
        "outputId": "1d9b3ec4-264a-4f1f-a6e2-c92780f98450"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/vicuna\n",
            "controller.log\t\t output        vicuna-7b-delta-v1.1\n",
            "model_worker_3efdd9.log  pyllama_data  vicuna_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 获取llama权重\n",
        "\n",
        "7B大小13G，13B大小25G，fp16"
      ],
      "metadata": {
        "id": "W6UiQZYMWgLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m llama.download --model_size 7B\n",
        "# !python -m llama.download --model_size 13B"
      ],
      "metadata": {
        "id": "7WlwR1R2WtCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a752062-fdf8-4b6f-9a4a-e214d4b8dc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❤️ Resume download is supported. You can ctrl-c and rerun the program to resume the downloading\n",
            "Downloading tokenizer...\n",
            "✅ pyllama_data/tokenizer.model\n",
            "✅ pyllama_data/tokenizer_checklist.chk\n",
            "tokenizer.model: OK\n",
            "Downloading 7B\n",
            "downloading file to pyllama_data/7B/consolidated.00.pth ...please wait for a few minutes ...\n",
            "✅ pyllama_data/7B/consolidated.00.pth\n",
            "✅ pyllama_data/7B/params.json\n",
            "✅ pyllama_data/7B/checklist.chk\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P ./pyllama_data https://raw.githubusercontent.com/huggingface/transformers/main/src/transformers/models/llama/convert_llama_weights_to_hf.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouB4H3pku9ca",
        "outputId": "015065ed-8159-4fcf-e166-8dcb227e481d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-03 17:32:35--  https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘./pyllama_data/convert_llama_weights_to_hf.py.1’\n",
            "\n",
            "convert_llama_weigh     [ <=>                ] 269.08K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-06-03 17:32:35 (7.62 MB/s) - ‘./pyllama_data/convert_llama_weights_to_hf.py.1’ saved [275542]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 生成羊驼模型"
      ],
      "metadata": {
        "id": "c5VTnF4ExBst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m fastchat.model.apply_delta --base ./output/7B --target vicuna_data/vicuna-7b-v1.1 --delta lmsys/vicuna-7b-delta-v1.1 --low-cpu-mem\n",
        "# !python3 -m fastchat.model.apply_delta --base ./pyllama_data/output/13B --target vicuna_data/vicuna-13b-v1.1 --delta lmsys/vicuna-13b-delta-v1.1"
      ],
      "metadata": {
        "id": "ascDih5av7MK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d67e307-bac8-4ad0-cce2-0754e261a979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-03 19:15:08.591570: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Split files for the base model to /tmp/tmpxbxtn6vl\n",
            "100% 2/2 [03:35<00:00, 107.53s/it]\n",
            "Split files for the delta weights to /tmp/tmphv6l4ic9\n",
            "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]\n",
            "Downloading (…)bc7a4/.gitattributes: 100% 1.48k/1.48k [00:00<00:00, 5.90MB/s]\n",
            "\n",
            "Downloading (…)3f3d0bc7a4/README.md: 100% 1.88k/1.88k [00:00<00:00, 7.86MB/s]\n",
            "Fetching 10 files: 100% 10/10 [00:00<00:00, 39.41it/s]\n",
            "100% 2/2 [02:37<00:00, 78.86s/it]\n",
            "Applying the delta\n",
            "4it [07:09, 107.45s/it]\n",
            "Saving the target model to vicuna_data/vicuna-7b-v1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 羊驼模型运行\n",
        "\n",
        "- 13B需要28G显存，无法直接使用，可以使用--load-8bit\n",
        "\n"
      ],
      "metadata": {
        "id": "n-2Qta3RxIOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m fastchat.serve.cli --model-path vicuna_data/vicuna-7b-v1.1\n",
        "# !python3 -m fastchat.serve.cli --model-path vicuna_data/vicuna-13b-v1.1 --load-8bit"
      ],
      "metadata": {
        "id": "l_NMBJqfxI0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe73767d-887c-4368-b4b7-42c5c4a034e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-03 19:44:09.793326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading checkpoint shards: 100% 4/4 [03:54<00:00, 58.55s/it]\n",
            "USER: 中国的首都在哪\n",
            "ASSISTANT: 中国的首府是北京。\n",
            "USER: exit...\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.10/\u001b[0m\u001b[1;33mrunpy.py\u001b[0m:\u001b[94m196\u001b[0m in \u001b[92m_run_module_as_main\u001b[0m                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m193 \u001b[0m\u001b[2m│   \u001b[0mmain_globals = sys.modules[\u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m].\u001b[91m__dict__\u001b[0m                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m194 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m alter_argv:                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   │   \u001b[0msys.argv[\u001b[94m0\u001b[0m] = mod_spec.origin                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m196 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _run_code(code, main_globals, \u001b[94mNone\u001b[0m,                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   │   │   │   │    \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m, mod_spec)                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mrun_module\u001b[0m(mod_name, init_globals=\u001b[94mNone\u001b[0m,                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.10/\u001b[0m\u001b[1;33mrunpy.py\u001b[0m:\u001b[94m86\u001b[0m in \u001b[92m_run_code\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 83 \u001b[0m\u001b[2m│   │   │   │   │      \u001b[0m__loader__ = loader,                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 84 \u001b[0m\u001b[2m│   │   │   │   │      \u001b[0m__package__ = pkg_name,                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 85 \u001b[0m\u001b[2m│   │   │   │   │      \u001b[0m__spec__ = mod_spec)                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 86 \u001b[2m│   \u001b[0mexec(code, run_globals)                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 87 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m run_globals                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 88 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_run_module_code\u001b[0m(code, init_globals=\u001b[94mNone\u001b[0m,                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fastchat/serve/\u001b[0m\u001b[1;33mcli.py\u001b[0m:\u001b[94m200\u001b[0m in         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m<module>\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   │   \u001b[0mhelp=\u001b[33m\"\u001b[0m\u001b[33mPrint useful debug information (e.g., prompts)\u001b[0m\u001b[33m\"\u001b[0m,         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m)                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0margs = parser.parse_args()                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mmain(args)                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mKeyboardInterrupt\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}